{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Parallel computation with Dask for InSAR applications\n",
    "\n",
    "A practical introduction to parallel computation for DePSI developers.\n",
    "\n",
    "<img src=\"figs/netherlands-escience-center-logo-RGB.svg\" alt=\"nlesc-logo\" width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Dask? \n",
    "\n",
    "Disclaimer:\n",
    "\n",
    "- It may not be faster than numpy;\n",
    "- It may not be more efficient than numpy;\n",
    "\n",
    "Dask is a solution for **Scaling Up** your computation with **Less pain**.\n",
    "\n",
    "- Operates with larger-than-memory data;\n",
    "- Efficiently use HPC resources;\n",
    "- Minimal changes to your numpy code;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask in a nutshell\n",
    "\n",
    "<img src=\"figs/dask-overview.svg\" alt=\"dask-overview\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure scheduler\n",
    "\n",
    "This is to tell Dask how to utilize the available resources.\n",
    "\n",
    "Check [Dask documentation for scheduling](https://docs.dask.org/en/stable/scheduling.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-machine scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "# threaded scheduler, usually the default option\n",
    "dask.config.set(scheduler='threads')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process scheduler, not recommended by dask. Use local cluster instead.\n",
    "dask.config.set(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force single-threaded execution, very useful for debugging\n",
    "dask.config.set(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a local cluster\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=2) # limit the concurrency resources to avoid memory issues\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a SLURM dask cluster\n",
    "# We use this in HPC with SLURM system\n",
    "\n",
    "from dask.distributed import SLURMCluster, Client\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    name=\"dask-worker\",  # Name of the Slurm job\n",
    "    queue=\"normal\", # Name of the node partition on your SLURM system\n",
    "    cores=4, # Number of cores per worker\n",
    "    memory=\"32 GB\",  # Total amount of memory per worker\n",
    "    processes=1,  # Number of Python processes per worker\n",
    "    walltime=\"3:00:00\",  # Reserve each worker for X hour\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: apply_gufunc\n",
    "\n",
    "[DePSI/slc.py](https://github.com/TUDelftGeodesy/DePSI/blob/main/depsi/slc.py) reconstruct SLC complex from ifg and mother complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ifg_to_slc(mother_slc, ifgs):\n",
    "    slc_out = ifgs.copy()\n",
    "    meta_arr = np.array((), dtype=np.complex64)\n",
    "    slc_complex = da.apply_gufunc(\n",
    "        _slc_complex_recontruct,\n",
    "        \"(),()->()\",\n",
    "        mother_slc[\"complex\"],\n",
    "        slc_out[\"complex\"],\n",
    "        meta=meta_arr,\n",
    "    )\n",
    "    slc_out = slc_out.assign({\"complex\": ((\"azimuth\", \"range\", \"time\"), slc_complex)})\n",
    "    return slc_out\n",
    "\n",
    "\n",
    "def _slc_complex_recontruct(mother_slc_complex, ifg_complex):\n",
    "    return ifg_complex / mother_slc_complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Example 2: map_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def enrich_from_polygon(self, polygon, fields, xlabel=\"lon\", ylabel=\"lat\"):\n",
    "    \"\"\"Enrich the SpaceTimeMatrix from one or more attribute fields of a (multi-)polygon.\n",
    "\n",
    "    Each attribute in fields will be assigned as a data variable to the STM.\n",
    "\n",
    "    If a point of the STM falls into the given polygon, the value of the specified field will\n",
    "    be added.\n",
    "\n",
    "    For space entries outside the (multi-)polygon, the value will be None.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polygon : geopandas.GeoDataFrame, str, or pathlib.Path\n",
    "        Polygon or multi-polygon with contextual information for enrichment\n",
    "    fields : str or list of str\n",
    "        Field name(s) in the (multi-)polygon for enrichment\n",
    "    xlabel : str, optional\n",
    "        Name of the x-coordinates of the STM, by default \"lon\"\n",
    "    ylabel : str, optional\n",
    "        Name of the y-coordinates of the STM, by default \"lat\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        Enriched STM.\n",
    "\n",
    "    \"\"\"\n",
    "    _ = _validate_coords(self._obj, xlabel, ylabel)\n",
    "\n",
    "    # Check if fields is a Iterable or a str\n",
    "    if isinstance(fields, str):\n",
    "        fields = [fields]\n",
    "    elif not isinstance(fields, Iterable):\n",
    "        raise ValueError(\"fields need to be a Iterable or a string\")\n",
    "\n",
    "    # Get polygon type and the first row\n",
    "    if isinstance(polygon, gpd.GeoDataFrame):\n",
    "        type_polygon = \"GeoDataFrame\"\n",
    "        polygon_one_row = polygon.iloc[0:1]\n",
    "    elif isinstance(polygon, Path | str):\n",
    "        type_polygon = \"File\"\n",
    "        polygon_one_row = gpd.read_file(polygon, rows=1)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Cannot recognize the input polygon.\")\n",
    "\n",
    "    # Check if fields exists in polygon\n",
    "    for field in fields:\n",
    "        if field not in polygon_one_row.columns:\n",
    "            raise ValueError(f'Field \"{field}\" not found in the the input polygon')\n",
    "\n",
    "    # Enrich all fields\n",
    "    ds = self._obj\n",
    "    chunks = (ds.chunksizes[\"space\"][0],)  # Assign an empty fields to ds\n",
    "    for field in fields:\n",
    "        ds = ds.assign(\n",
    "            {\n",
    "                field: (\n",
    "                    [\"space\"],\n",
    "                    da.from_array(np.full(ds.space.shape, None), chunks=chunks),\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    ds = xr.map_blocks(\n",
    "        _enrich_from_polygon_block,\n",
    "        ds,\n",
    "        args=(polygon, fields, xlabel, ylabel, type_polygon),\n",
    "        template=ds,\n",
    "    )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _enrich_from_polygon_block(ds, polygon, fields, xlabel, ylabel, type_polygon):\n",
    "    \"\"\"Block-wise function for \"enrich_from_polygon\".\"\"\"\n",
    "    # Get the match list\n",
    "    match_list, polygon = _ml_str_query(ds[xlabel], ds[ylabel], polygon, type_polygon)\n",
    "\n",
    "    _ds = ds.copy(deep=True)\n",
    "\n",
    "    if match_list.ndim == 2:\n",
    "        intuids = np.unique(match_list[:, 0])\n",
    "        for intuid in intuids:\n",
    "            intm = np.where(match_list[:, 0] == intuid)[0]\n",
    "            intmid = match_list[intm, 1]\n",
    "            for field in fields:\n",
    "                _ds[field].data[intmid] = polygon.iloc[intuid][field]\n",
    "\n",
    "    return _ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Example 3: groupby + map + map_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_batch_one_group(stm_pnt, slc_quality_ref, h2ph_ref):\n",
    "    \n",
    "    # stm_pnt = stm_pnt.compute()\n",
    "    slc_quality_pnts = stm_pnt['slc_quality']\n",
    "    h2ph_pnts = stm_pnt['h2ph_values']\n",
    "    dd_arc = stm_pnt['dd_complex']\n",
    "    # # Compute the diagonal of the VCM of the dd phases\n",
    "    Qyy_diagonal = np.sqrt((slc_quality_ref)**2 + (slc_quality_pnts)**2)\n",
    "\n",
    "\n",
    "    # Compute the variance covariance matrix of the DD based on the NMAD for the arc\n",
    "    Qyy = np.identity(len(stm_pnt.time))*Qyy_diagonal.to_numpy()**2\n",
    "\n",
    "    # Compute 'mean' h2ph value for the arc (which we currently model as the average of the two time series)\n",
    "    \n",
    "    h2ph_arc = (h2ph_ref + h2ph_pnts)/2\n",
    "    # h2ph_arc = h2ph_arc.to_numpy()\n",
    "    h2ph_arc = h2ph_arc.squeeze().values\n",
    "\n",
    "    # Get the wrapped phase\n",
    "    phs_wrapped = np.angle(dd_arc)\n",
    "    phs_wrapped = phs_wrapped.squeeze()\n",
    "\n",
    "    # Define y and the corresponding VQM Qyy\n",
    "    # y = np.append(phs_wrapped, [0, 0, 0, 0])\n",
    "    Q_phs = Qyy\n",
    "    Q_b = np.diag([sigma_offset**2, sigma_vel**2, sigma_h**2, sigma_ther**2])\n",
    "    # Qyy = np.block([[Q_phs, np.zeros((Q_phs.shape[0], Q_b.shape[1]))],\n",
    "    #                 [np.zeros((Q_b.shape[0], Q_phs.shape[1])), Q_b]])\n",
    "\n",
    "    # Define the design matrices\n",
    "    A1 = np.diag([-2*np.pi] * len(phs_wrapped))\n",
    "    # B2 = np.diag([1] * 4)\n",
    "    # C = np.block([[A1, np.zeros((A1.shape[0], B2.shape[1]))],\n",
    "    #                 [np.zeros((B2.shape[0], A1.shape[1])), B2]])\n",
    "\n",
    "    B1 = np.ones((phs_wrapped.shape[0],4))\n",
    "    B1[:,1] = years*(-4*np.pi/wavelength/1000)\n",
    "    B1[:,2] = h2ph_arc*(-4*np.pi/wavelength)\n",
    "    B1[:,3] = temp*(-4*np.pi/wavelength/1000)\n",
    "    # C[:len(phs_wrapped), -4:] = B1\n",
    "\n",
    "    # Float solution with least-squares\n",
    "    ahat = phs_wrapped/(-2*np.pi)\n",
    "    Qahat = 1/(4*((np.pi)**2)) * (Q_phs + B1@Q_b@B1.T)\n",
    "\n",
    "\n",
    "    # Lambda method - Integer bootstrapping\n",
    "    afixed,sqnorm,Ps,Qzhat,Z,nfixed,mu = LAMBDA.main(ahat,Qahat,3)\n",
    "\n",
    "    # Calculate the unwrapped phase [rad]\n",
    "    phs_unw  = phs_wrapped - A1@afixed\n",
    "    \n",
    "    # Get the estimated parameters\n",
    "    b_hat = np.linalg.inv((B1.T@np.linalg.inv(Q_phs)@B1))@B1.T@np.linalg.inv(Q_phs)@phs_unw\n",
    "    \n",
    "    # Get the phase for estimated DD observation, non-thermal displacement, height difference and thermal expension [rad]\n",
    "    phs_est = B1@b_hat\n",
    "    phs_dis =  B1[:,:2]@b_hat[:2]\n",
    "    phs_height = B1[:,2]*b_hat[2]\n",
    "    phs_ther = B1[:,3]*b_hat[3]\n",
    "\n",
    "    # Get the phase for estimated DD observation, non-thermal displacement, height difference and thermal expension [mm]\n",
    "    dis_est = phs_est*(-wavelength/(4*np.pi)*1000)\n",
    "    dis_dis = phs_dis*(-wavelength/(4*np.pi)*1000)\n",
    "    dis_height = phs_height*(-wavelength/(4*np.pi)*1000)\n",
    "    dis_ther = phs_ther*(-wavelength/(4*np.pi)*1000)\n",
    "\n",
    "    # Get the wrapped and unwrapped phase [mm]\n",
    "    dis_wrapped = phs_wrapped*(-wavelength/(4*np.pi)*1000)\n",
    "    dis_unw = phs_unw*(-wavelength/(4*np.pi)*1000)\n",
    "\n",
    "    ds_out = stm_pnt.copy()\n",
    "    ds_out = ds_out.assign(dis_wrapped=((ds_out.dims), np.expand_dims(dis_wrapped, axis=0)))\n",
    "    ds_out = ds_out.assign(dis_unw=((ds_out.dims), np.expand_dims(dis_unw, axis=0)))\n",
    "    ds_out = ds_out.assign(dis_est=((ds_out.dims), np.expand_dims(dis_est, axis=0)))\n",
    "    ds_out = ds_out.assign(dis_dis=((ds_out.dims), np.expand_dims(dis_dis, axis=0)))\n",
    "    ds_out = ds_out.assign(dis_height=((ds_out.dims), np.expand_dims(dis_height, axis=0)))\n",
    "    ds_out = ds_out.assign(dis_ther=((ds_out.dims), np.expand_dims(dis_ther, axis=0)))\n",
    "\n",
    "    ds_out['vel'] = xr.DataArray(np.array([b_hat[1]]), dims=('space'), coords={'space': ds_out.space.values})\n",
    "    ds_out['height'] = xr.DataArray(np.array([b_hat[2]]), dims=('space'), coords={'space': ds_out.space.values})\n",
    "    ds_out['ther'] = xr.DataArray(np.array([b_hat[3]]), dims=('space'), coords={'space': ds_out.space.values})\n",
    "\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_batch_chunk(ds, sd_complex_ref, slc_quality_ref, h2ph_ref):\n",
    "    \"\"\"\n",
    "    Given temporal differnces of point i and point j we compute the double difference phase \n",
    "    We also compute the quality of the arc time series \n",
    "\n",
    "    point i is the reference point and is subtracted from point j:\n",
    "    sd_complex_conj_i = sd_complex_i.conj()\n",
    "    dd_arc = sd_complex_j*sd_complex_conj_i\n",
    "\n",
    "    Remark that the output of the quality is the sigma of the DD phase. \n",
    "\n",
    "    Example:\n",
    "        dd_arc, Qyy_diagonal_sigma = compute_dd(sd_complex_i, sd_complex_j,sd_quality_i, sd_quality_j, plot, dates, mother_idx)\n",
    "\n",
    "\n",
    "    Code is as follows:\n",
    "        sd_complex_conj_i = sd_complex_i.conj()\n",
    "        dd_arc = sd_complex_j*sd_complex_conj_i\n",
    "       \n",
    "        Qyy_diagonal_sigma = np.sqrt((sd_quality_i)**2 + (sd_quality_j)**2)\n",
    "    \"\"\"\n",
    "    ds_out = ds.copy()\n",
    "    slc_complex_pnts =  ds['sd_complex']\n",
    "    # slc_quality_pnts = ds['slc_quality']\n",
    "    sd_complex_conj_ref = sd_complex_ref.conj()\n",
    "    dd_arc = slc_complex_pnts*sd_complex_conj_ref\n",
    "\n",
    "    # # # Compute the diagonal of the VCM of the dd phases\n",
    "    # Qyy_diagonal = np.sqrt((slc_quality_ref)**2 + (slc_quality_pnts)**2)\n",
    "\n",
    "    ds_out['dd_complex'] = dd_arc\n",
    "    # ds_out['Qyy_diagonal'] = Qyy_diagonal\n",
    "    # ds_out['h2ph_arc'] = h2ph_arc\n",
    "\n",
    "    # # Load chunk in memory\n",
    "    # ds_out = ds_out.compute()\n",
    "\n",
    "    groups = ds_out.groupby(\"space\")\n",
    "    ds_analysis = groups.map(\n",
    "        full_batch_one_group,\n",
    "        slc_quality_ref = slc_quality_ref, \n",
    "        h2ph_ref = h2ph_ref\n",
    "    )\n",
    "\n",
    "    return ds_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stm_new2 = xr.map_blocks(full_batch_chunk, stm_new_in, kwargs = {\"sd_complex_ref\": sd_complex_ref_com, \"slc_quality_ref\": slc_quality_ref_com, \"h2ph_ref\": h2ph_ref_com}, template = stm_new_in)\n",
    "stm_new2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydepsi-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
